# LLM-Guideline

## What is tokenization, and why is it important in LLMs?
Tokenization is the process of splitting text into smaller units called tokens, which can be words, subwords, or even characters. For instance, the word "tokenization" might be broken down into smaller subwords like "token" and "ization". This step is crucial because LLMs do not understand raw text directly. Instead, they process sequences of numbers that represents these tokens.

Effective tokenization allows models to handle various languages, manage rare words, and reduce the vocabulary size, which improves both efficiency and performance.

